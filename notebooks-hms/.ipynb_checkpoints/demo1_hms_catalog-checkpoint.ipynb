{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: HMS Catalog with Apache Iceberg\n",
    "\n",
    "This lesson demonstrates using **Hive Metastore (HMS)** with Apache Iceberg.\n",
    "\n",
    "**Architecture:**\n",
    "- Spark (Query Engine)\n",
    "- Hive Metastore via Thrift (Catalog)\n",
    "- PostgreSQL (HMS backing database - private network)\n",
    "- MinIO (S3-compatible storage)\n",
    "\n",
    "**Network Configuration:**\n",
    "- HMS: `hive_metastore:9083` (internal Docker network)\n",
    "- MinIO: `minio_hms:9000` (internal Docker network)\n",
    "- Host can access MinIO at `localhost:9400` (port mapping)\n",
    "\n",
    "**Benefits:**\n",
    "- Production-ready\n",
    "- Industry standard\n",
    "- Compatible with existing Hive infrastructure\n",
    "- ACID guarantees\n",
    "- Easy migration to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Spark with HMS Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop config explicitly set\n",
      "  HMS URI: thrift://hive-metastore:9083\n",
      "  S3 endpoint: minio-hms:9000\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "MINIO_HOST = \"minio-hms\"  # DNS-safe alias for MinIO container\n",
    "MINIO_PORT = \"9000\"  # Internal container port (NOT 9400 which is host port)\n",
    "MINIO_ENDPOINT = f\"{MINIO_HOST}:{MINIO_PORT}\"\n",
    "MINIO_HTTP_ENDPOINT = f\"http://{MINIO_HOST}:{MINIO_PORT}\"\n",
    "WAREHOUSE_PATH = \"s3a://warehouse\"\n",
    "\n",
    "# HMS configuration\n",
    "HMS_HOST = \"hive-metastore\"  # Docker service name for Hive Metastore\n",
    "HMS_PORT = \"9083\"  # Internal container port\n",
    "HMS_URI = f\"thrift://{HMS_HOST}:{HMS_PORT}\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('iceberg_hms_catalog')\n",
    "        .set('spark.jars.packages', \n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'org.apache.iceberg:iceberg-hive-metastore:1.5.0,'\n",
    "             'org.apache.hadoop:hadoop-aws:3.3.4,'\n",
    "             'com.amazonaws:aws-java-sdk-bundle:1.12.262'\n",
    "            )\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "        \n",
    "        # HMS Catalog Configuration (Iceberg)\n",
    "        .set('spark.sql.catalog.spark_catalog', \n",
    "             'org.apache.iceberg.spark.SparkSessionCatalog')\n",
    "        .set('spark.sql.catalog.spark_catalog.type', 'hive')\n",
    "        .set('spark.sql.catalog.spark_catalog.uri', HMS_URI)\n",
    "        .set('spark.sql.catalog.spark_catalog.warehouse', WAREHOUSE_PATH)\n",
    "        .set('spark.sql.catalog.spark_catalog.io-impl',\n",
    "             'org.apache.iceberg.hadoop.HadoopFileIO')\n",
    "\n",
    "        # HMS Configuration (Hadoop/Spark Hive Client)\n",
    "        .set('spark.hadoop.hive.metastore.uris', HMS_URI)\n",
    "        .set('spark.hadoop.hive.metastore.client.connect.retry.delay', '5')\n",
    "        .set('spark.hadoop.hive.metastore.client.socket.timeout', '1800')\n",
    "        \n",
    "        # MinIO S3 Configuration for Iceberg S3FileIO (AWS SDK v2)\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.endpoint', MINIO_HTTP_ENDPOINT)\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.path-style-access', 'true')\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.access-key-id', MINIO_ACCESS_KEY)\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.secret-access-key', MINIO_SECRET_KEY)\n",
    "        \n",
    "        # MinIO S3 Configuration for Hadoop s3a filesystem\n",
    "        .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.endpoint', MINIO_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "        .set(\"spark.sql.warehouse.dir\", \"s3a://warehouse\")                  # Spark default\n",
    "        .set(\"spark.sql.catalog.spark_catalog.warehouse\", \"s3a://warehouse\") # Iceberg catalog\n",
    "        .set(\"spark.hadoop.hive.metastore.warehouse.dir\", \"s3a://warehouse\") # Hive\n",
    ")\n",
    "\n",
    "# Force stop any existing session first\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# NOW create new session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# ALWAYS set Hadoop config explicitly (this is the critical part)\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"hive.metastore.uris\", HMS_URI)\n",
    "hadoop_conf.set(\"hive.metastore.client.connect.retry.delay\", \"5\")\n",
    "hadoop_conf.set(\"hive.metastore.client.socket.timeout\", \"1800\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "print(\"Hadoop config explicitly set\")\n",
    "print(f\"  HMS URI: {hadoop_conf.get('hive.metastore.uris')}\")\n",
    "print(f\"  S3 endpoint: {hadoop_conf.get('fs.s3a.endpoint')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connectivity Tests\n",
    "\n",
    "Run these tests to verify HMS and MinIO are working before creating tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 118\n",
      "Hostname: a3b111b7aa3f\n",
      "Python DNS: 172.23.0.3\n",
      "172.23.0.3      hive_metastore\n",
      "# Generated by Docker Engine.\n",
      "# This file can be edited; Docker Engine will not make further changes once it\n",
      "# has been modified.\n",
      "\n",
      "nameserver 127.0.0.11\n",
      "search local\n",
      "options ndots:0\n",
      "\n",
      "# Based on host file: '/etc/resolv.conf' (internal resolver)\n",
      "# ExtServers: [host(10.0.0.109) host(1.1.1.1) host(2001:558:feed::1) host(2001:558:feed::2)]\n",
      "# Overrides: []\n",
      "# Option ndots from: internal\n"
     ]
    }
   ],
   "source": [
    "import socket, os, subprocess, sys\n",
    "\n",
    "print(\"PID:\", os.getpid())\n",
    "print(\"Hostname:\", os.uname().nodename)\n",
    "\n",
    "print(\"Python DNS:\", socket.gethostbyname(\"hive_metastore\"))\n",
    "\n",
    "# shell commands from the same kernel\n",
    "print(subprocess.check_output([\"getent\", \"hosts\", \"hive_metastore\"]).decode().strip())\n",
    "print(subprocess.check_output([\"cat\", \"/etc/resolv.conf\"]).decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONNECTIVITY TESTS\n",
      "============================================================\n",
      "\n",
      "1. DNS Resolution:\n",
      "   OK: hive_metastore -> 172.23.0.3\n",
      "   OK: minio-hms -> 172.23.0.2\n",
      "\n",
      "2. Port Connectivity:\n",
      "   OK: hive_metastore:9083\n",
      "   OK: minio-hms:9000\n",
      "\n",
      "3. HMS Connection:\n",
      "   OK: Connected (1 namespace(s))\n",
      "\n",
      "4. Hadoop Configuration:\n",
      "   hive.metastore.uris: thrift://hive-metastore:9083\n",
      "   fs.s3a.endpoint: minio-hms:9000\n",
      "\n",
      "5. S3A Write/Read Test:\n",
      "   OK: Write and read successful (count: 1)\n",
      "\n",
      "6. HMS + Iceberg + S3 Integration:\n",
      "hive.metastore.uris = thrift://hive-metastore:9083\n",
      "spark.sql.catalog.spark_catalog.uri = thrift://hive-metastore:9083\n",
      "   OK: HMS + Iceberg + S3 working. Result: success\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONNECTIVITY TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Network DNS Resolution\n",
    "print(\"\\n1. DNS Resolution:\")\n",
    "for service in ['hive_metastore', 'minio-hms']:\n",
    "    try:\n",
    "        ip = socket.gethostbyname(service)\n",
    "        print(f\"   OK: {service} -> {ip}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   FAIL: {service}: {e}\")\n",
    "\n",
    "# Test 2: Port Connectivity\n",
    "print(\"\\n2. Port Connectivity:\")\n",
    "for service, port in [('hive_metastore', 9083), ('minio-hms', 9000)]:\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(5)\n",
    "        result = sock.connect_ex((service, port))\n",
    "        sock.close()\n",
    "        status = \"OK\" if result == 0 else \"FAIL\"\n",
    "        print(f\"   {status}: {service}:{port}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   FAIL: {service}:{port}: {e}\")\n",
    "\n",
    "# Test 3: HMS from Spark\n",
    "print(\"\\n3. HMS Connection:\")\n",
    "try:\n",
    "    ns = spark.sql(\"SHOW NAMESPACES\").collect()\n",
    "    print(f\"   OK: Connected ({len(ns)} namespace(s))\")\n",
    "except Exception as e:\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 4: Hadoop Config Check\n",
    "print(\"\\n4. Hadoop Configuration:\")\n",
    "hconf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "print(f\"   hive.metastore.uris: {hconf.get('hive.metastore.uris') or 'NOT SET'}\")\n",
    "print(f\"   fs.s3a.endpoint: {hconf.get('fs.s3a.endpoint') or 'NOT SET'}\")\n",
    "\n",
    "# Test 5: S3A Write/Read\n",
    "print(\"\\n5. S3A Write/Read Test:\")\n",
    "try:\n",
    "    test_df = spark.createDataFrame([(1, \"test\")], [\"id\", \"val\"])\n",
    "    test_path = \"s3a://warehouse/_test\"\n",
    "    test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "    count = spark.read.parquet(test_path).count()\n",
    "    print(f\"   OK: Write and read successful (count: {count})\")\n",
    "except Exception as e:\n",
    "    print(f\"   FAIL: {str(e)[:100]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Test 6: Create Iceberg Table\n",
    "print(\"\\n6. HMS + Iceberg + S3 Integration:\")\n",
    "\n",
    "hconf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "print(\"hive.metastore.uris =\", hconf.get(\"hive.metastore.uris\"))\n",
    "print(\"spark.sql.catalog.spark_catalog.uri =\", spark.conf.get(\"spark.sql.catalog.spark_catalog.uri\", \"NOT SET\"))\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS spark_catalog.default._test_table\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE spark_catalog.default._test_table (id INT, msg STRING)\n",
    "        USING iceberg\n",
    "        LOCATION 's3a://warehouse/_test_table'\n",
    "    \"\"\")\n",
    "    spark.sql(\"INSERT INTO spark_catalog.default._test_table VALUES (1, 'success')\")\n",
    "    result = spark.sql(\"SELECT * FROM spark_catalog.default._test_table\").collect()\n",
    "    spark.sql(\"DROP TABLE spark_catalog.default._test_table\")\n",
    "    print(f\"   OK: HMS + Iceberg + S3 working. Result: {result[0].msg}\")\n",
    "except Exception as e:\n",
    "    print(f\"   FAIL: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13126 rows\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId| competitorName|firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|      affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       47661|     Dan Bailey|      Dan|  Bailey|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|    CrossFit Legacy| 27|  NULL|  NULL|          1|          43|       1|2011|\n",
      "|      124483| Joshua Bridges|   Joshua| Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|  CrossFit Invictus| 28|  NULL|  NULL|          2|          44|       1|2011|\n",
      "|       11435|   Rich Froning|     Rich| Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|     CrossFit Faith| 23|  NULL|  NULL|          3|          61|       1|2011|\n",
      "|      151906|     Mikko Salo|    Mikko|    Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|          0|      CrossFit Pori| 31|  NULL|  NULL|          4|          75|       1|2011|\n",
      "|       10169|Austin Malleolo|   Austin|Malleolo|   ACT|     M|               NULL|               NULL|      11|         North East|          0|Reebok CrossFit One| 24|  NULL|  NULL|          5|         112|       1|2011|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create default namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS spark_catalog.default\")\n",
    "\n",
    "# Load CSV data\n",
    "df_2011 = spark.read.csv(\n",
    "    \"../datasets/df_open_2011.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_2011.createOrReplaceTempView(\"csv_open_2011\")\n",
    "\n",
    "print(f\"Loaded {df_2011.count()} rows\")\n",
    "df_2011.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Iceberg Table with HMS + MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created Iceberg table: spark_catalog.default.df_open_2011_hms\n"
     ]
    }
   ],
   "source": [
    "# Create Iceberg table from CSV data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_catalog.default.df_open_2011_hms\n",
    "    USING iceberg \n",
    "    AS SELECT * FROM csv_open_2011\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Created Iceberg table: spark_catalog.default.df_open_2011_hms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query the Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|      competitorName|firstName|            lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|      affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       47661|          Dan Bailey|      Dan|              Bailey|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|    CrossFit Legacy| 27|  NULL|  NULL|          1|          43|       1|2011|\n",
      "|      124483|      Joshua Bridges|   Joshua|             Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|  CrossFit Invictus| 28|  NULL|  NULL|          2|          44|       1|2011|\n",
      "|       11435|        Rich Froning|     Rich|             Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|     CrossFit Faith| 23|  NULL|  NULL|          3|          61|       1|2011|\n",
      "|      151906|          Mikko Salo|    Mikko|                Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|          0|      CrossFit Pori| 31|  NULL|  NULL|          4|          75|       1|2011|\n",
      "|       10169|     Austin Malleolo|   Austin|            Malleolo|   ACT|     M|               NULL|               NULL|      11|         North East|          0|Reebok CrossFit One| 24|  NULL|  NULL|          5|         112|       1|2011|\n",
      "|        5284|     Daniel Tyminski|   Daniel|            Tyminski|   ACT|     M|               NULL|               NULL|      11|         North East|          0|               NULL| 25|  NULL|  NULL|          6|         157|       1|2011|\n",
      "|       24156|         NEAL MADDOX|     Neal|              Maddox|   ACT|     M|               NULL|               NULL|      12|Northern California|          0|               NULL| 33|  NULL|  NULL|          7|         164|       1|2011|\n",
      "|       39857|        Todd Edmunds|     Todd|             Edmunds|   ACT|     M|               NULL|               NULL|       9|       Mid Atlantic|          0|    R.A.W. Training| 34|  NULL|  NULL|          8|         177|       1|2011|\n",
      "|       35231|         Rob Downton|      Rob|             Downton|   ACT|     M|               NULL|               NULL|       3|          Australia|          0|   CrossFit NorWest| 31|  NULL|  NULL|          9|         247|       1|2011|\n",
      "|       29334|\"Jarett \"\"Ninja\"\"...|   Jarett|\"\"\"Ninja\"\" Perelm...|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|     Brick CrossFit| 36|  NULL|  NULL|         10|         257|       1|2011|\n",
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|13126|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_hms LIMIT 10\").show()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_hms\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Table in HMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|         |csv_open_2011|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n",
      "+-------------------+---------+-------+\n",
      "|col_name           |data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|competitorId       |int      |NULL   |\n",
      "|competitorName     |string   |NULL   |\n",
      "|firstName          |string   |NULL   |\n",
      "|lastName           |string   |NULL   |\n",
      "|status             |string   |NULL   |\n",
      "|gender             |string   |NULL   |\n",
      "|countryOfOriginCode|string   |NULL   |\n",
      "|countryOfOriginName|string   |NULL   |\n",
      "|regionId           |int      |NULL   |\n",
      "|regionName         |string   |NULL   |\n",
      "|affiliateId        |int      |NULL   |\n",
      "|affiliateName      |string   |NULL   |\n",
      "|age                |int      |NULL   |\n",
      "|height             |string   |NULL   |\n",
      "|weight             |string   |NULL   |\n",
      "|overallRank        |int      |NULL   |\n",
      "|overallScore       |int      |NULL   |\n",
      "|genderId           |int      |NULL   |\n",
      "|year               |int      |NULL   |\n",
      "|                   |         |       |\n",
      "+-------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all tables in HMS\n",
    "spark.sql(\"SHOW TABLES IN spark_catalog.default\").show()\n",
    "\n",
    "# Show table details\n",
    "spark.sql(\"DESCRIBE EXTENDED spark_catalog.default.df_open_2011_hms\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. HMS Catalog Connectivity Tests\n",
    "\n",
    "Test HMS catalog's connections to its dependencies (PostgreSQL and MinIO).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HMS CATALOG CONNECTIVITY TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Can HMS reach PostgreSQL?\n",
    "print(\"\\n1. HMS -> PostgreSQL Connection:\")\n",
    "print(\"   HMS uses PostgreSQL at postgres_hms:5432 (private network)\")\n",
    "try:\n",
    "    # Try to resolve postgres_hms (may not be accessible from notebook container)\n",
    "    try:\n",
    "        ip = socket.gethostbyname('postgres_hms')\n",
    "        print(f\"   OK: Can resolve postgres_hms -> {ip}\")\n",
    "        # Try to connect\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(2)\n",
    "        result = sock.connect_ex(('postgres_hms', 5432))\n",
    "        sock.close()\n",
    "        if result == 0:\n",
    "            print(\"   OK: PostgreSQL port 5432 is accessible from notebook\")\n",
    "        else:\n",
    "            print(\"   INFO: PostgreSQL not directly accessible (private network)\")\n",
    "            print(\"         Expected: HMS can access it, notebook cannot\")\n",
    "    except socket.gaierror:\n",
    "        print(\"   INFO: Cannot resolve postgres_hms from notebook container\")\n",
    "        print(\"         Expected: postgres_hms is on HMS private network\")\n",
    "except Exception as e:\n",
    "    print(f\"   INFO: {e}\")\n",
    "\n",
    "# Test 2: Verify HMS is running and accessible\n",
    "print(\"\\n2. HMS Service Health:\")\n",
    "try:\n",
    "    # Check if we can connect to HMS Thrift port\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(5)\n",
    "    result = sock.connect_ex(('hive-metastore', 9083))\n",
    "    sock.close()\n",
    "    if result == 0:\n",
    "        print(\"   OK: HMS Thrift service (9083) is running\")\n",
    "    else:\n",
    "        print(\"   FAIL: HMS Thrift service not accessible\")\n",
    "    \n",
    "    # Test HMS via Spark\n",
    "    spark.sql(\"SHOW NAMESPACES\").collect()\n",
    "    print(\"   OK: HMS responds to Spark queries\")\n",
    "    \n",
    "    # Check if HMS can list tables (means it connected to PostgreSQL)\n",
    "    tables = spark.sql(\"SHOW TABLES IN spark_catalog.default\").collect()\n",
    "    print(f\"   OK: HMS can query metadata ({len(tables)} tables)\")\n",
    "    print(\"   OK: HMS -> PostgreSQL connection works\")\n",
    "except Exception as e:\n",
    "    print(f\"   FAIL: HMS health check failed: {e}\")\n",
    "\n",
    "# Test 3: HMS -> MinIO Connection\n",
    "print(\"\\n3. HMS -> MinIO S3 Connection:\")\n",
    "print(\"   HMS configured with:\")\n",
    "print(\"   - HIVE_METASTORE_WAREHOUSE_DIR=s3a://warehouse/\")\n",
    "print(\"   - AWS_ACCESS_KEY_ID=admin\")\n",
    "print(\"   - MinIO endpoint via spark.hadoop.fs.s3a.endpoint\")\n",
    "\n",
    "try:\n",
    "    # Create a table to force HMS to interact with S3\n",
    "    spark.sql(\"DROP TABLE IF EXISTS spark_catalog.default._hms_s3_test\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE spark_catalog.default._hms_s3_test (\n",
    "            id INT,\n",
    "            test STRING\n",
    "        )\n",
    "        USING iceberg\n",
    "        LOCATION 's3a://warehouse/test/_hms_s3_test'\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert data (this will write to MinIO via Iceberg)\n",
    "    spark.sql(\"INSERT INTO spark_catalog.default._hms_s3_test VALUES (1, 'hms-s3-test')\")\n",
    "    \n",
    "    # Read back (HMS must read metadata from S3)\n",
    "    result = spark.sql(\"SELECT * FROM spark_catalog.default._hms_s3_test\").collect()\n",
    "    \n",
    "    # Check table location in HMS metadata\n",
    "    location_df = spark.sql(\"\"\"\n",
    "        DESCRIBE EXTENDED spark_catalog.default._hms_s3_test\n",
    "    \"\"\").filter(\"col_name = 'Location'\")\n",
    "    location = location_df.collect()[0].data_type\n",
    "    \n",
    "    print(f\"   OK: Created table with S3 location: {location}\")\n",
    "    print(\"   OK: Wrote data to MinIO via Iceberg\")\n",
    "    print(f\"   OK: Read data back: {result[0].test}\")\n",
    "    print(\"   OK: HMS -> MinIO connection working\")\n",
    "    \n",
    "    # Cleanup\n",
    "    spark.sql(\"DROP TABLE spark_catalog.default._hms_s3_test\")\n",
    "    print(\"   OK: Test table cleaned up\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   FAIL: HMS -> MinIO test failed: {e}\")\n",
    "    import traceback\n",
    "    print(f\"   Error: {traceback.format_exc()[:300]}\")\n",
    "\n",
    "# Test 4: Verify HMS Warehouse Configuration\n",
    "print(\"\\n4. HMS Warehouse Configuration:\")\n",
    "try:\n",
    "    # Check if HMS warehouse dir is set correctly\n",
    "    result = spark.sql(\"DESCRIBE EXTENDED spark_catalog.default\").filter(\n",
    "        \"col_name = 'Catalog'\").collect()\n",
    "    \n",
    "    print(f\"   Catalog type: {result[0].data_type if result else 'N/A'}\")\n",
    "    \n",
    "    # Check a table's location to verify S3 path\n",
    "    tables = spark.sql(\"SHOW TABLES IN spark_catalog.default\").collect()\n",
    "    if len(tables) > 0:\n",
    "        first_table = tables[0].tableName\n",
    "        loc_result = spark.sql(f\"\"\"\n",
    "            DESCRIBE EXTENDED spark_catalog.default.{first_table}\n",
    "        \"\"\").filter(\"col_name = 'Location'\").collect()\n",
    "        \n",
    "        if loc_result:\n",
    "            location = loc_result[0].data_type\n",
    "            print(f\"   Sample table location: {location}\")\n",
    "            if location.startswith(\"s3a://warehouse\"):\n",
    "                print(\"   OK: Tables are using correct S3 warehouse path\")\n",
    "            else:\n",
    "                print(\"   INFO: Unexpected warehouse path\")\n",
    "except Exception as e:\n",
    "    print(f\"   INFO: {e}\")\n",
    "\n",
    "# Test 5: Network Topology Summary\n",
    "print(\"\\n5. Network Topology Summary:\")\n",
    "print(\"   [Notebook] -> [hive-metastore] -> [postgres_hms]\")\n",
    "print(\"   [Notebook] -> [minio-hms]\")\n",
    "print(\"   [hive-metastore] -> [minio-hms]\")\n",
    "print(\"   [hive-metastore] -> [postgres_hms] (private network)\")\n",
    "print(\"   OK: HMS can reach PostgreSQL and MinIO\")\n",
    "print(\"   OK: Notebook can reach HMS and MinIO\")\n",
    "print(\"   OK: Notebook cannot reach PostgreSQL (expected)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULT: HMS catalog has required connections\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
