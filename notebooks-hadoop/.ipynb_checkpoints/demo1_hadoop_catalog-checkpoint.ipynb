{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Hadoop Catalog with Apache Iceberg\n",
    "\n",
    "This lesson demonstrates using the **Hadoop Catalog** with Apache Iceberg backed by **Backblaze B2** (S3-compatible storage).\n",
    "\n",
    "**Architecture:**\n",
    "- Spark (Query Engine)\n",
    "- Hadoop Catalog (Iceberg metadata in object storage)\n",
    "- Backblaze B2 (S3-compatible storage)\n",
    "\n",
    "**Benefits:**\n",
    "- Production-ready for S3-compatible object storage\n",
    "- Simple setup with no external catalog service\n",
    "- ACID guarantees with Iceberg tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Spark with Hadoop Catalog and Backblaze B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Backblaze B2 (S3-compatible) configuration\n",
    "B2_ACCESS_KEY_ID = os.getenv(\"B2_APPLICATION_KEY_ID\")\n",
    "B2_SECRET_KEY = os.getenv(\"B2_APPLICATION_KEY\")\n",
    "B2_ENDPOINT = os.getenv(\"B2_S3_ENDPOINT\", \"s3.us-east-005.backblazeb2.com\")\n",
    "B2_REGION = os.getenv(\"B2_REGION\", \"us-east-005\")\n",
    "B2_BUCKET = os.getenv(\"B2_BUCKET\", \"iceberg-test\")\n",
    "WAREHOUSE_PATH = f\"s3a://{B2_BUCKET}/warehouse\"\n",
    "\n",
    "if not B2_ACCESS_KEY_ID or not B2_SECRET_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing Backblaze B2 credentials. Set B2_APPLICATION_KEY_ID and B2_APPLICATION_KEY.\"\n",
    "    )\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName(\"iceberg_hadoop_catalog\")\n",
    "        .set(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.11.375\"\n",
    "        )\n",
    "        .set(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "        )\n",
    "\n",
    "        # Hadoop Catalog configuration\n",
    "        .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "        .set(\"spark.sql.catalog.spark_catalog.warehouse\", WAREHOUSE_PATH)\n",
    "        .set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "        # Hadoop S3A configuration for Backblaze B2\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", B2_ENDPOINT)\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", B2_ACCESS_KEY_ID)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", B2_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    "        )\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark running with Iceberg Hadoop Catalog\")\n",
    "print(f\"B2 endpoint: {B2_ENDPOINT}\")\n",
    "print(f\"Warehouse: {WAREHOUSE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Default Namespace\n",
    "\n",
    "Hadoop Catalog starts empty. Create the `default` namespace first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default namespace if it doesn't exist\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS spark_catalog.default\")\n",
    "print(\"Created default namespace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Catalog Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available databases\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Show existing tables\n",
    "spark.sql(\"SHOW TABLES IN spark_catalog.default\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into temporary view\n",
    "csv_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2011.csv\")\n",
    "csv_df.createOrReplaceTempView(\"csv_open_2011\")\n",
    "\n",
    "print(f\"Loaded {csv_df.count()} rows from CSV\")\n",
    "csv_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Iceberg Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iceberg table from CSV data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_catalog.default.df_open_2011_hadoop\n",
    "    USING iceberg\n",
    "    AS SELECT * FROM csv_open_2011\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created Iceberg table: spark_catalog.default.df_open_2011_hadoop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Iceberg Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_hadoop LIMIT 10\").show()\n",
    "\n",
    "# Count records\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_hadoop\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ACID Operations - Delete Records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records (ACID transaction)\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM spark_catalog.default.df_open_2011_hadoop\n",
    "    WHERE gender = 'M'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Deleted records\")\n",
    "\n",
    "# Verify deletion\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_hadoop\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Travel and History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table history\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_hadoop.history\").show(truncate=False)\n",
    "\n",
    "# Show snapshots\n",
    "spark.sql(\n",
    "    \"SELECT snapshot_id, parent_id, operation FROM \"\n",
    "    \"spark_catalog.default.df_open_2011_hadoop.snapshots\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the two most recent snapshots for time travel\n",
    "history_df = spark.sql(\"\"\"\n",
    "    SELECT snapshot_id\n",
    "    FROM spark_catalog.default.df_open_2011_hadoop.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 2\n",
    "\"\"\")\n",
    "\n",
    "rows = history_df.collect()\n",
    "if len(rows) >= 2:\n",
    "    current_id = rows[0][0]\n",
    "    previous_id = rows[1][0]\n",
    "\n",
    "    print(f\"Current ID: {current_id}\")\n",
    "    print(f\"Previous ID: {previous_id}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count_previous\n",
    "        FROM spark_catalog.default.df_open_2011_hadoop\n",
    "        VERSION AS OF {previous_id}\n",
    "    \"\"\").show()\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count_current\n",
    "        FROM spark_catalog.default.df_open_2011_hadoop\n",
    "        VERSION AS OF {current_id}\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Not enough history to find a previous version.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Table Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table schema\n",
    "spark.sql(\"DESCRIBE spark_catalog.default.df_open_2011_hadoop\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
