{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Hadoop Catalog with Apache Iceberg\n",
    "\n",
    "This lesson demonstrates using the **Hadoop Catalog** with Apache Iceberg backed by **Backblaze B2** (S3-compatible storage).\n",
    "\n",
    "**Architecture:**\n",
    "- Spark (Query Engine)\n",
    "- Hadoop Catalog (Iceberg metadata in object storage)\n",
    "- Backblaze B2 (S3-compatible storage)\n",
    "\n",
    "**Benefits:**\n",
    "- Production-ready for S3-compatible object storage\n",
    "- Simple setup with no external catalog service\n",
    "- ACID guarantees with Iceberg tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Spark with Hadoop Catalog and Backblaze B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark running with Iceberg Hadoop Catalog\n",
      "B2 endpoint: s3.us-east-005.backblazeb2.com\n",
      "Warehouse: s3a://iceberg-test/warehouse\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Backblaze B2 (S3-compatible) configuration\n",
    "B2_ACCESS_KEY_ID = os.getenv(\"B2_APPLICATION_KEY_ID\")\n",
    "B2_SECRET_KEY = os.getenv(\"B2_APPLICATION_KEY\")\n",
    "B2_ENDPOINT = os.getenv(\"B2_S3_ENDPOINT\", \"s3.us-east-005.backblazeb2.com\")\n",
    "B2_REGION = os.getenv(\"B2_REGION\", \"us-east-005\")\n",
    "B2_BUCKET = os.getenv(\"B2_BUCKET\", \"iceberg-test\")\n",
    "WAREHOUSE_PATH = f\"s3a://{B2_BUCKET}/warehouse\"\n",
    "\n",
    "if not B2_ACCESS_KEY_ID or not B2_SECRET_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing Backblaze B2 credentials. Set B2_APPLICATION_KEY_ID and B2_APPLICATION_KEY.\"\n",
    "    )\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName(\"iceberg_hadoop_catalog\")\n",
    "        .set(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.11.375\"\n",
    "        )\n",
    "        .set(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "        )\n",
    "\n",
    "        # Hadoop Catalog configuration\n",
    "        .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "        .set(\"spark.sql.catalog.spark_catalog.warehouse\", WAREHOUSE_PATH)\n",
    "        .set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "        # Hadoop S3A configuration for Backblaze B2\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", B2_ENDPOINT)\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", B2_ACCESS_KEY_ID)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", B2_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    "        )\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark running with Iceberg Hadoop Catalog\")\n",
    "print(f\"B2 endpoint: {B2_ENDPOINT}\")\n",
    "print(f\"Warehouse: {WAREHOUSE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Default Namespace\n",
    "\n",
    "Hadoop Catalog starts empty. Create the `default` namespace first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created default namespace\n"
     ]
    }
   ],
   "source": [
    "# Create default namespace if it doesn't exist\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS spark_catalog.default\")\n",
    "print(\"Created default namespace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Catalog Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show available databases\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Show existing tables\n",
    "spark.sql(\"SHOW TABLES IN spark_catalog.default\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13126 rows from CSV\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId| competitorName|firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|      affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       47661|     Dan Bailey|      Dan|  Bailey|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|    CrossFit Legacy| 27|  NULL|  NULL|          1|          43|       1|2011|\n",
      "|      124483| Joshua Bridges|   Joshua| Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|  CrossFit Invictus| 28|  NULL|  NULL|          2|          44|       1|2011|\n",
      "|       11435|   Rich Froning|     Rich| Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|     CrossFit Faith| 23|  NULL|  NULL|          3|          61|       1|2011|\n",
      "|      151906|     Mikko Salo|    Mikko|    Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|          0|      CrossFit Pori| 31|  NULL|  NULL|          4|          75|       1|2011|\n",
      "|       10169|Austin Malleolo|   Austin|Malleolo|   ACT|     M|               NULL|               NULL|      11|         North East|          0|Reebok CrossFit One| 24|  NULL|  NULL|          5|         112|       1|2011|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV into temporary view\n",
    "csv_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2011.csv\")\n",
    "csv_df.createOrReplaceTempView(\"csv_open_2011\")\n",
    "\n",
    "print(f\"Loaded {csv_df.count()} rows from CSV\")\n",
    "csv_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Iceberg Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Iceberg table: spark_catalog.default.df_open_2011_hadoop\n"
     ]
    }
   ],
   "source": [
    "# Create Iceberg table from CSV data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_catalog.default.df_open_2011_hadoop\n",
    "    USING iceberg\n",
    "    AS SELECT * FROM csv_open_2011\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created Iceberg table: spark_catalog.default.df_open_2011_hadoop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Iceberg Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|      competitorName|firstName|            lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|      affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       47661|          Dan Bailey|      Dan|              Bailey|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|    CrossFit Legacy| 27|  NULL|  NULL|          1|          43|       1|2011|\n",
      "|      124483|      Joshua Bridges|   Joshua|             Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|  CrossFit Invictus| 28|  NULL|  NULL|          2|          44|       1|2011|\n",
      "|       11435|        Rich Froning|     Rich|             Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|     CrossFit Faith| 23|  NULL|  NULL|          3|          61|       1|2011|\n",
      "|      151906|          Mikko Salo|    Mikko|                Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|          0|      CrossFit Pori| 31|  NULL|  NULL|          4|          75|       1|2011|\n",
      "|       10169|     Austin Malleolo|   Austin|            Malleolo|   ACT|     M|               NULL|               NULL|      11|         North East|          0|Reebok CrossFit One| 24|  NULL|  NULL|          5|         112|       1|2011|\n",
      "|        5284|     Daniel Tyminski|   Daniel|            Tyminski|   ACT|     M|               NULL|               NULL|      11|         North East|          0|               NULL| 25|  NULL|  NULL|          6|         157|       1|2011|\n",
      "|       24156|         NEAL MADDOX|     Neal|              Maddox|   ACT|     M|               NULL|               NULL|      12|Northern California|          0|               NULL| 33|  NULL|  NULL|          7|         164|       1|2011|\n",
      "|       39857|        Todd Edmunds|     Todd|             Edmunds|   ACT|     M|               NULL|               NULL|       9|       Mid Atlantic|          0|    R.A.W. Training| 34|  NULL|  NULL|          8|         177|       1|2011|\n",
      "|       35231|         Rob Downton|      Rob|             Downton|   ACT|     M|               NULL|               NULL|       3|          Australia|          0|   CrossFit NorWest| 31|  NULL|  NULL|          9|         247|       1|2011|\n",
      "|       29334|\"Jarett \"\"Ninja\"\"...|   Jarett|\"\"\"Ninja\"\" Perelm...|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|     Brick CrossFit| 36|  NULL|  NULL|         10|         257|       1|2011|\n",
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|13126|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_hadoop LIMIT 10\").show()\n",
    "\n",
    "# Count records\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_hadoop\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ACID Operations - Delete Records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted records\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "| 4506|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete records (ACID transaction)\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM spark_catalog.default.df_open_2011_hadoop\n",
    "    WHERE gender = 'M'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Deleted records\")\n",
    "\n",
    "# Verify deletion\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_hadoop\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Travel and History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-21 19:00:11.634|3693390167868883144|NULL               |true               |\n",
      "|2026-01-21 19:00:22.316|8648452601893943335|3693390167868883144|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "+-------------------+-------------------+---------+\n",
      "|snapshot_id        |parent_id          |operation|\n",
      "+-------------------+-------------------+---------+\n",
      "|3693390167868883144|NULL               |append   |\n",
      "|8648452601893943335|3693390167868883144|overwrite|\n",
      "+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show table history\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_hadoop.history\").show(truncate=False)\n",
    "\n",
    "# Show snapshots\n",
    "spark.sql(\n",
    "    \"SELECT snapshot_id, parent_id, operation FROM \"\n",
    "    \"spark_catalog.default.df_open_2011_hadoop.snapshots\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the two most recent snapshots for time travel\n",
    "history_df = spark.sql(\"\"\"\n",
    "    SELECT snapshot_id\n",
    "    FROM spark_catalog.default.df_open_2011_hadoop.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 2\n",
    "\"\"\")\n",
    "\n",
    "rows = history_df.collect()\n",
    "if len(rows) >= 2:\n",
    "    current_id = rows[0][0]\n",
    "    previous_id = rows[1][0]\n",
    "\n",
    "    print(f\"Current ID: {current_id}\")\n",
    "    print(f\"Previous ID: {previous_id}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count_previous\n",
    "        FROM spark_catalog.default.df_open_2011_hadoop\n",
    "        VERSION AS OF {previous_id}\n",
    "    \"\"\").show()\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count_current\n",
    "        FROM spark_catalog.default.df_open_2011_hadoop\n",
    "        VERSION AS OF {current_id}\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Not enough history to find a previous version.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Table Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table schema\n",
    "spark.sql(\"DESCRIBE spark_catalog.default.df_open_2011_hadoop\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
