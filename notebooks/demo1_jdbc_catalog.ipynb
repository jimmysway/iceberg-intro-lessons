{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: JDBC Catalog with Apache Iceberg\n",
    "\n",
    "This lesson demonstrates using **JDBC Catalog** with Apache Iceberg.\n",
    "\n",
    "**Architecture:**\n",
    "- Spark (Query Engine)\n",
    "- PostgreSQL (JDBC Catalog)\n",
    "- MinIO (S3-compatible storage)\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Production-ready\n",
    "- ✅ Simple setup\n",
    "- ✅ ACID guarantees\n",
    "- ✅ Easy migration to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Spark with JDBC Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running with Iceberg and JDBC Catalog\n",
      "Catalog: JDBC (PostgreSQL at postgres:5432)\n",
      "Storage: MinIO at minio:9000\n",
      "Warehouse: s3a://warehouse\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "MINIO_HOST = \"minio\"  # Use Docker service name (resolved via internal DNS)\n",
    "WAREHOUSE_PATH = \"s3a://warehouse\"\n",
    "\n",
    "# PostgreSQL (JDBC Catalog) configuration\n",
    "POSTGRES_USER = \"iceberg\"\n",
    "POSTGRES_PASSWORD = \"iceberg\"\n",
    "POSTGRES_DB = \"iceberg\"\n",
    "POSTGRES_HOST = \"postgres\"\n",
    "POSTGRES_PORT = \"5432\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('iceberg_jdbc_catalog')\n",
    "        .set('spark.jars.packages', \n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "             'software.amazon.awssdk:bundle:2.20.26,'\n",
    "             'software.amazon.awssdk:url-connection-client:2.20.26,'\n",
    "             'org.postgresql:postgresql:42.7.1')\n",
    "        .set('spark.sql.extensions', \n",
    "             'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "        \n",
    "        # JDBC Catalog Configuration\n",
    "        .set('spark.sql.catalog.spark_catalog', \n",
    "             'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.spark_catalog.catalog-impl', \n",
    "             'org.apache.iceberg.jdbc.JdbcCatalog')\n",
    "        .set('spark.sql.catalog.spark_catalog.uri', \n",
    "             f'jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}')\n",
    "        .set('spark.sql.catalog.spark_catalog.jdbc.user', POSTGRES_USER)\n",
    "        .set('spark.sql.catalog.spark_catalog.jdbc.password', POSTGRES_PASSWORD)\n",
    "        .set('spark.sql.catalog.spark_catalog.warehouse', WAREHOUSE_PATH)\n",
    "        .set('spark.sql.catalog.spark_catalog.io-impl', \n",
    "             'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        \n",
    "        # MinIO S3 Configuration for Iceberg S3FileIO (AWS SDK v2)\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.endpoint', f'http://{MINIO_HOST}:9000')\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.path-style-access', 'true')\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.access-key-id', MINIO_ACCESS_KEY)\n",
    "        .set('spark.sql.catalog.spark_catalog.s3.secret-access-key', MINIO_SECRET_KEY)\n",
    "        \n",
    "        # MinIO S3 Configuration for Hadoop s3a filesystem (if needed)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.endpoint', f'http://{MINIO_HOST}:9000')\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running with Iceberg and JDBC Catalog\")\n",
    "print(f\"Catalog: JDBC (PostgreSQL at {POSTGRES_HOST}:{POSTGRES_PORT})\")\n",
    "print(f\"Storage: MinIO at {MINIO_HOST}:9000\")\n",
    "print(f\"Warehouse: {WAREHOUSE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Default Namespace\n",
    "\n",
    "JDBC Catalog starts empty - we need to create the `default` namespace first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created default namespace\n"
     ]
    }
   ],
   "source": [
    "# Create default namespace if it doesn't exist\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS spark_catalog.default\")\n",
    "print(\"✓ Created default namespace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Catalog Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show available databases\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Show existing tables\n",
    "spark.sql(\"SHOW TABLES IN spark_catalog.default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13126 rows from CSV\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId| competitorName|firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|      affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       47661|     Dan Bailey|      Dan|  Bailey|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|    CrossFit Legacy| 27|  NULL|  NULL|          1|          43|       1|2011|\n",
      "|      124483| Joshua Bridges|   Joshua| Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|  CrossFit Invictus| 28|  NULL|  NULL|          2|          44|       1|2011|\n",
      "|       11435|   Rich Froning|     Rich| Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|     CrossFit Faith| 23|  NULL|  NULL|          3|          61|       1|2011|\n",
      "|      151906|     Mikko Salo|    Mikko|    Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|          0|      CrossFit Pori| 31|  NULL|  NULL|          4|          75|       1|2011|\n",
      "|       10169|Austin Malleolo|   Austin|Malleolo|   ACT|     M|               NULL|               NULL|      11|         North East|          0|Reebok CrossFit One| 24|  NULL|  NULL|          5|         112|       1|2011|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV into temporary view\n",
    "csv_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2011.csv\")\n",
    "csv_df.createOrReplaceTempView(\"csv_open_2011\")\n",
    "\n",
    "print(f\"Loaded {csv_df.count()} rows from CSV\")\n",
    "csv_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Create Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created Iceberg table: spark_catalog.default.df_open_2011_jdbc\n"
     ]
    }
   ],
   "source": [
    "# Create Iceberg table from CSV data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_catalog.default.df_open_2011_jdbc \n",
    "    USING iceberg \n",
    "    AS SELECT * FROM csv_open_2011\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Created Iceberg table: spark_catalog.default.df_open_2011_jdbc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|      competitorName|firstName|            lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|      affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       47661|          Dan Bailey|      Dan|              Bailey|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|    CrossFit Legacy| 27|  NULL|  NULL|          1|          43|       1|2011|\n",
      "|      124483|      Joshua Bridges|   Joshua|             Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|  CrossFit Invictus| 28|  NULL|  NULL|          2|          44|       1|2011|\n",
      "|       11435|        Rich Froning|     Rich|             Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|          0|     CrossFit Faith| 23|  NULL|  NULL|          3|          61|       1|2011|\n",
      "|      151906|          Mikko Salo|    Mikko|                Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|          0|      CrossFit Pori| 31|  NULL|  NULL|          4|          75|       1|2011|\n",
      "|       10169|     Austin Malleolo|   Austin|            Malleolo|   ACT|     M|               NULL|               NULL|      11|         North East|          0|Reebok CrossFit One| 24|  NULL|  NULL|          5|         112|       1|2011|\n",
      "|        5284|     Daniel Tyminski|   Daniel|            Tyminski|   ACT|     M|               NULL|               NULL|      11|         North East|          0|               NULL| 25|  NULL|  NULL|          6|         157|       1|2011|\n",
      "|       24156|         NEAL MADDOX|     Neal|              Maddox|   ACT|     M|               NULL|               NULL|      12|Northern California|          0|               NULL| 33|  NULL|  NULL|          7|         164|       1|2011|\n",
      "|       39857|        Todd Edmunds|     Todd|             Edmunds|   ACT|     M|               NULL|               NULL|       9|       Mid Atlantic|          0|    R.A.W. Training| 34|  NULL|  NULL|          8|         177|       1|2011|\n",
      "|       35231|         Rob Downton|      Rob|             Downton|   ACT|     M|               NULL|               NULL|       3|          Australia|          0|   CrossFit NorWest| 31|  NULL|  NULL|          9|         247|       1|2011|\n",
      "|       29334|\"Jarett \"\"Ninja\"\"...|   Jarett|\"\"\"Ninja\"\" Perelm...|   ACT|     M|               NULL|               NULL|      16|Southern California|          0|     Brick CrossFit| 36|  NULL|  NULL|         10|         257|       1|2011|\n",
      "+------------+--------------------+---------+--------------------+------+------+-------------------+-------------------+--------+-------------------+-----------+-------------------+---+------+------+-----------+------------+--------+----+\n",
      "\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|13126|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_jdbc LIMIT 10\").show()\n",
    "\n",
    "# Count records\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_jdbc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Uploading multiple Dataframes and Adding to Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into temporary view\n",
    "df_2011 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2011.csv\")\n",
    "df_2012 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2012.csv\")\n",
    "df_2013 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2013.csv\")\n",
    "\n",
    "\n",
    "df_2011.writeTo(\"spark_catalog.default.multi_year_iceberg\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .create()\n",
    "\n",
    "# 2. Add the next DataFrames\n",
    "df_2012.writeTo(\"spark_catalog.default.multi_year_iceberg\").append()\n",
    "df_2013.writeTo(\"spark_catalog.default.multi_year_iceberg\").append()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two different ways we added Iceberg tables.\n",
    "\n",
    "In **4a.** we added an Iceberg table using an SQL statement something with CTAS (Create Table as SELECT) using `spark.sql()`\n",
    "\n",
    "In **6.** we used the `writeTo()` API which is a more python-like approach to build a table. Notice we also appended the data of 2012 and 2013 into the Iceberg Table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can query the entire table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+--------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|  competitorName|firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|         regionName|affiliateId|       affiliateName|age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+----------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+--------------------+---+------+------+-----------+------------+--------+----+\n",
      "|       11435|    Rich Froning|     Rich| Froning|   ACT|     M|               NULL|               NULL|       6|       Central East|       3220|     CrossFit Mayhem| 25|  NULL|  NULL|          1|          28|       1|2013|\n",
      "|      151906|      Mikko Salo|    Mikko|    Salo|   ACT|     M|               NULL|               NULL|       7|             Europe|       3128|                NULL| 33|  NULL|  NULL|          2|          53|       1|2013|\n",
      "|      124483|    Josh Bridges|     Josh| Bridges|   ACT|     M|               NULL|               NULL|      16|Southern California|        600|   CrossFit Invictus| 30|  NULL|  NULL|          3|          59|       1|2013|\n",
      "|       34796|   Scott Panchik|    Scott| Panchik|   ACT|     M|               NULL|               NULL|       6|       Central East|       1814|                NULL| 25|  NULL|  NULL|          4|          84|       1|2013|\n",
      "|       21343|Kenneth Leverich|  Kenneth|Leverich|   ACT|     M|               NULL|               NULL|      16|Southern California|       1700|                NULL| 25|  NULL|  NULL|          5|         113|       1|2013|\n",
      "|       24156|     Neal Maddox|     Neal|  Maddox|   ACT|     M|               NULL|               NULL|      12|Northern California|       3606|CrossFit X-treme ...| 35|  NULL|  NULL|          6|         122|       1|2013|\n",
      "|       12485|   Jason Khalipa|    Jason| Khalipa|   ACT|     M|               NULL|               NULL|      12|Northern California|       2605|                NULL| 27|  NULL|  NULL|          7|         156|       1|2013|\n",
      "|        5284| Daniel Tyminski|   Daniel|Tyminski|   ACT|     M|               NULL|               NULL|      11|         North East|       2016|                NULL| 27|  NULL|  NULL|          8|         214|       1|2013|\n",
      "|       11446|Dominick Maurici| Dominick| Maurici|   ACT|     M|               NULL|               NULL|      15|         South East|       2960|Caution CrossFit ...| 24|  NULL|  NULL|          9|         217|       1|2013|\n",
      "|        5642|      Matt Baird|     Matt|   Baird|   ACT|     M|               NULL|               NULL|      15|         South East|        399|                NULL| 27|  NULL|  NULL|         10|         272|       1|2013|\n",
      "+------------+----------------+---------+--------+------+------+-------------------+-------------------+--------+-------------------+-----------+--------------------+---+------+------+-----------+------------+--------+----+\n",
      "\n",
      "+------+\n",
      "| total|\n",
      "+------+\n",
      "|137182|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM spark_catalog.default.multi_year_iceberg LIMIT 10\").show()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.multi_year_iceberg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ACID Operations - Delete Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deleted records\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "| 4506|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete records (ACID transaction)\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM spark_catalog.default.df_open_2011_jdbc \n",
    "    WHERE gender = 'M'\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Deleted records\")\n",
    "\n",
    "# Verify deletion\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM spark_catalog.default.df_open_2011_jdbc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Travel & History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-15 18:09:12.043|4644534369010550327|NULL               |true               |\n",
      "|2026-01-15 18:09:14.69 |6346540157899538446|4644534369010550327|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show table history\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.df_open_2011_jdbc.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------+\n",
      "|snapshot_id        |parent_id          |operation|\n",
      "+-------------------+-------------------+---------+\n",
      "|4644534369010550327|NULL               |append   |\n",
      "|6346540157899538446|4644534369010550327|overwrite|\n",
      "+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show snapshots\n",
    "spark.sql(\"SELECT snapshot_id, parent_id, operation FROM spark_catalog.default.df_open_2011_jdbc.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ID: 6346540157899538446\n",
      "Previous ID: 4644534369010550327\n",
      "+--------------+\n",
      "|count_previous|\n",
      "+--------------+\n",
      "|         13126|\n",
      "+--------------+\n",
      "\n",
      "+-------------+\n",
      "|count_current|\n",
      "+-------------+\n",
      "|         4506|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Run the query to get the 2 most recent snapshots\n",
    "# Row 0 = Current (Latest), Row 1 = Previous\n",
    "history_df = spark.sql(\"\"\"\n",
    "    SELECT snapshot_id\n",
    "    FROM spark_catalog.default.df_open_2011_jdbc.snapshots \n",
    "    ORDER BY committed_at DESC \n",
    "    LIMIT 2\n",
    "\"\"\")\n",
    "\n",
    "# 2. Collect the data to the driver as a list of Row objects\n",
    "rows = history_df.collect()\n",
    "\n",
    "# 3. Access the data using list indexing\n",
    "# Make sure we have at least 2 rows (a history)\n",
    "if len(rows) >= 2:\n",
    "    current_id = rows[0][0]  # First row, First column (Latest)\n",
    "    previous_id = rows[1][0] # Second row, First column (Previous)\n",
    "\n",
    "    print(f\"Current ID: {current_id}\")\n",
    "    print(f\"Previous ID: {previous_id}\")\n",
    "\n",
    "    # 4. Query using the previous ID\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count_previous \n",
    "        FROM spark_catalog.default.df_open_2011_jdbc \n",
    "        VERSION AS OF {previous_id}\n",
    "    \"\"\").show()\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count_current \n",
    "        FROM spark_catalog.default.df_open_2011_jdbc \n",
    "        VERSION AS OF {current_id}\n",
    "    \"\"\").show()\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough history to find a previous version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|       competitorId|   string|   NULL|\n",
      "|     competitorName|   string|   NULL|\n",
      "|          firstName|   string|   NULL|\n",
      "|           lastName|   string|   NULL|\n",
      "|             status|   string|   NULL|\n",
      "|             gender|   string|   NULL|\n",
      "|countryOfOriginCode|   string|   NULL|\n",
      "|countryOfOriginName|   string|   NULL|\n",
      "|           regionId|   string|   NULL|\n",
      "|         regionName|   string|   NULL|\n",
      "|        affiliateId|   string|   NULL|\n",
      "|      affiliateName|   string|   NULL|\n",
      "|                age|   string|   NULL|\n",
      "|             height|   string|   NULL|\n",
      "|             weight|   string|   NULL|\n",
      "|        overallRank|   string|   NULL|\n",
      "|       overallScore|   string|   NULL|\n",
      "|           genderId|   string|   NULL|\n",
      "|               year|   string|   NULL|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show table schema\n",
    "spark.sql(\"DESCRIBE spark_catalog.default.df_open_2011_jdbc\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
